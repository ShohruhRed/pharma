import pandas as pd
import torch
import numpy as np
from sanfis import SANFIS
from sklearn.metrics import classification_report
import pickle
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ ML-–¥–∞—Ç–∞—Å–µ—Ç–∞ ===
data = pd.read_csv("../data/ml_dataset_balanced.csv")
X_train = data.drop(columns=["batch_id", "stage", "defect_prob", "is_defect"]).values
y_train = data["is_defect"].values.astype(int)  # –ú–µ—Ç–∫–∏ –¥–µ—Ñ–µ–∫—Ç–æ–≤

# === 2. –í—ã–¥–µ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É ===
X_test = X_train[:300]  # –ü–µ—Ä–≤—ã–µ 300 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
y_test = y_train[:300]

# === 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º SMOTE ===
class_counts = np.bincount(y_train)
print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø–µ—Ä–µ–¥ SMOTE: {class_counts}")

if class_counts[1] < class_counts[0]:  # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã, –µ—Å–ª–∏ –¥–µ—Ñ–µ–∫—Ç—ã –≤ –º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–µ
    smote = SMOTE(sampling_strategy="minority", random_state=42)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
    print(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ SMOTE: {np.bincount(y_train_balanced)}")
else:
    X_train_balanced, y_train_balanced = X_train, y_train

# === 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ===
def normalize_data(X, train_min, train_max):
    return (X - train_min) / (train_max - train_min)

train_min = X_train_balanced.min(axis=0)
train_max = X_train_balanced.max(axis=0)
X_train_balanced = normalize_data(X_train_balanced, train_min, train_max)
X_test = normalize_data(X_test, train_min, train_max)

# === 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ===
np.save("../models/train_min.npy", train_min)
np.save("../models/train_max.npy", train_max)

# === 6. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã ===
X_train_balanced = torch.tensor(X_train_balanced, dtype=torch.float32)
y_train_balanced = torch.tensor(y_train_balanced.reshape(-1, 1), dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)

# === 7. –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏-–±–∞—Ç—á–∏ ===
batch_size = 32
train_loader = torch.utils.data.DataLoader(list(zip(X_train_balanced, y_train_balanced)), batch_size=batch_size, shuffle=True)

# === 8. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ SANFIS ===
input_dim = X_train_balanced.shape[1]
membfuncs = [
    {
        'function': 'gaussian',
        'n_memb': 5,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        'params': {
            'mu': {'value': np.linspace(0, 1, 5).tolist(), 'trainable': True},
            'sigma': {'value': [0.15] * 5, 'trainable': True}
        }
    }
] * input_dim

model = SANFIS(membfuncs=membfuncs, n_input=input_dim)

# === 9. –ò—Å–ø–æ–ª—å–∑—É–µ–º `Adam` –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ===
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
loss_fn = torch.nn.BCEWithLogitsLoss()

# === 10. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (EPOCHS = 300) ===
EPOCHS = 300
for epoch in range(EPOCHS):
    model.train()

    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(X_batch=X_batch, S_batch=X_batch)
        loss = loss_fn(output, y_batch.float())

        if torch.isnan(loss):
            print("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á...")
            optimizer.zero_grad()
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        optimizer.step()

    if (epoch + 1) % 50 == 0:
        print(f"Epoch {epoch + 1}/{EPOCHS} - Loss: {loss.item():.4f}")

# === 11. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ ===
threshold = 0.35
model.eval()
with torch.no_grad():
    y_pred_sanfis = torch.sigmoid(model(X_batch=X_test, S_batch=X_test)).numpy()
y_pred_ml = data["defect_prob"][:300].values  # –ü—Ä–æ–≥–Ω–æ–∑—ã ML

# === 12. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π SANFIS vs ML ===
comparison_df = pd.DataFrame({
    "ML_pred": y_pred_ml,
    "SANFIS_pred": y_pred_sanfis.flatten(),
    "Real_defect": y_test.numpy().flatten()
})

print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π SANFIS vs ML:")
print(comparison_df.head(15))  # –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 15 —Å—Ç—Ä–æ–∫

# === 13. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π ===
plt.figure(figsize=(8, 4))
plt.scatter(y_pred_ml, y_pred_sanfis, c=y_test.numpy().flatten(), cmap="coolwarm", alpha=0.7)
plt.xlabel("ML (defect_prob)")
plt.ylabel("SANFIS (defect probability)")
plt.title("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π SANFIS vs ML")
plt.colorbar(label="–ò—Å—Ç–∏–Ω–Ω—ã–π –¥–µ—Ñ–µ–∫—Ç (0 = –Ω–µ—Ç, 1 = –¥–∞)")
plt.savefig("../models/sanfis_vs_ml.png")
print("üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π SANFIS vs ML —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")

# === 14. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
torch.save(model.state_dict(), "../models/sanfis_model.pt")
with open("../models/sanfis_membfuncs.pkl", "wb") as f:
    pickle.dump(membfuncs, f)

print("‚úÖ SANFIS –æ–±—É—á–µ–Ω –Ω–∞ ML-–¥–∞–Ω–Ω—ã—Ö, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ML –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
